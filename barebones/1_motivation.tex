%-JPC: My concrete advice would be to leverage the playful first mini-paragraph that you have, and expand it into a more general statement of your research goals. So you don't think that Transformers are the endgame, so what does this mean? It means you're going to develop models to push the Pareto frontier of context length vs associative recall!
%-JPC:
%-JPC: So the order of things here should really be:
%-JPC: 1. Opening section that lays out your research interests
%-JPC: 2. "Designing new architectures will require understanding the limitations of current models.": This sentence, or something like it, to act as a transition into talking about limitations of current models
%-JPC: 3. The big background paragraphs you've written

%-TG: past motivation for linear-rnn models might not hold anymore + lay out research interests.
Are Transformers the endgame?\footnote{Sasha's ``Transformers in 2027'' bet: \url{https://www.isattentionallyouneed.com}.} I think not.
%
Are linear-recurrence models? I am unsure!
%
What I do know is that scaling context lengths while maintaining efficiency and associative recall is crucial in advancing current language models.
%
My research goals are twofold:
\begin{enumerate}[itemsep=-2pt, topsep=-1pt]
    \item analyzing the Pareto frontier of the context length vs. associative recall across varying hidden-state capacities, and
    \item developing models to push this frontier, achieving efficient and reliable long-context modeling.
\end{enumerate}

%-TG: transition into talking about limitations of current models.
Understanding the limitations of current models is essential to developing new architectures.
%
Recent advances, such as Flash-$\ast$ algorithms \citep{fu_flashfftconv_2023,shah_flashattention-3_2024} and compute-optimal training \citep{muennighoff_scaling_2023}, have made the quadratic complexity of Transformers less prohibitive for pretraining.
%
The real ``brick wall,'' however, lies in high inference costs, especially for models with implicit chain-of-thought (CoT) that require stepwise internalization.
%
Even with speculative decoding and KV-cache optimizations, using attention for very large contexts (e.g., $100$M tokens\footnote{Proprietary approaches like LTM-$2$-mini's \textit{sequence-dimension algorithm} offer glimpses of future possibilities but remain opaque: \url{https://magic.dev/blog/100m-token-context-windows}.}) is infeasible.

Efforts to distill pretrained Transformers to linear-recurrence models \citep{zhang_hedgehog_2024,wang_mamba_2024,zhang_lolcats_2024,bick_transformers_2024} seem promising but raise critical questions:
%
Do these distilled models inherit the same limitations as their base models, particularly in handling long contexts?
%
How expressive can they be given the fixed-size hidden memory constraints \citep{jelassi_repeat_2024}?
%
The recent Based model \citep{arora_simple_2024} builds on prior efforts by combining linear attention for larger hidden states with sliding-window attention for recall.
%
However, its long-context retrieval performance (e.g., in needle-in-a-haystack, hash-hop\footnote{HashHop long-context evaluation: \url{https://github.com/magicproduct/hash-hop}.}) remains underexplored.
%
In our experiments, RecurrentGemma-$9$B---a moderately-sized model with a $4,096$-dimensional hidden state and a $2,048$-token local attention window---struggles beyond $4\times$ its local window size in needle-in-a-haystack tasks.
%
To this end, we need to devise mechanisms that reduce token storage redundancy (unlike Transformers) that support dynamic memory scaling.
