
@inproceedings{chang_trouble_2019,
	title = {Trouble on the {Horizon}: {Forecasting} the {Derailment} of {Online} {Conversations} as they {Develop}},
	shorttitle = {Trouble on the {Horizon}},
	url = {https://aclanthology.org/D19-1481},
	abstract = {Online discussions often derail into toxic exchanges between participants. Recent efforts mostly focused on detecting antisocial behavior after the fact, by analyzing single comments in isolation. To provide more timely notice to human moderators, a system needs to preemptively detect that a conversation is heading towards derailment before it actually turns toxic. This means modeling derailment as an emerging property of a conversation rather than as an isolated utterance-level event. Forecasting emerging conversational properties, however, poses several inherent modeling challenges. First, since conversations are dynamic, a forecasting model needs to capture the flow of the discussion, rather than properties of individual comments. Second, real conversations have an unknown horizon: they can end or derail at any time; thus a practical forecasting model needs to assess the risk in an online fashion, as the conversation develops. In this work we introduce a conversational forecasting model that learns an unsupervised representation of conversational dynamics and exploits it to predict future derailment as the conversation develops. By applying this model to two new diverse datasets of online conversations with labels for antisocial events, we show that it outperforms state-of-the-art systems at forecasting derailment.},
	urldate = {2024-11-16},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	author = {Chang, Jonathan P. and Danescu-Niculescu-Mizil, Cristian},
	month = nov,
	year = {2019},
	pages = {4743--4754},
}

@misc{wang_mamba_2024,
	title = {The {Mamba} in the {Llama}: {Distilling} and {Accelerating} {Hybrid} {Models}},
	shorttitle = {The {Mamba} in the {Llama}},
	url = {http://arxiv.org/abs/2408.15237},
	doi = {10.48550/arXiv.2408.15237},
	abstract = {Linear RNN architectures, like Mamba, can be competitive with Transformer models in language modeling while having advantageous deployment characteristics. Given the focus on training large-scale Transformer models, we consider the challenge of converting these pretrained models for deployment. We demonstrate that it is feasible to distill large Transformers into linear RNNs by reusing the linear projection weights from attention layers with academic GPU resources. The resulting hybrid model, which incorporates a quarter of the attention layers, achieves performance comparable to the original Transformer in chat benchmarks and outperforms open-source hybrid Mamba models trained from scratch with trillions of tokens in both chat benchmarks and general benchmarks. Moreover, we introduce a hardware-aware speculative decoding algorithm that accelerates the inference speed of Mamba and hybrid models. Overall we show how, with limited computation resources, we can remove many of the original attention layers and generate from the resulting model more efficiently. Our top-performing model, distilled from Llama3-8B-Instruct, achieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and 7.35 on MT-Bench, surpassing the best instruction-tuned linear RNN model.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Wang, Junxiong and Paliotta, Daniele and May, Avner and Rush, Alexander M. and Dao, Tri},
	month = aug,
	year = {2024},
	note = {arXiv:2408.15237},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{arora_simple_2024,
	title = {Simple linear attention language models balance the recall-throughput tradeoff},
	url = {http://arxiv.org/abs/2402.18668},
	doi = {10.48550/arXiv.2402.18668},
	abstract = {Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: https://github.com/HazyResearch/based.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Arora, Simran and Eyuboglu, Sabri and Zhang, Michael and Timalsina, Aman and Alberti, Silas and Zinsley, Dylan and Zou, James and Rudra, Atri and Ré, Christopher},
	month = feb,
	year = {2024},
	note = {arXiv:2402.18668},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{muennighoff_scaling_2023,
	title = {Scaling {Data}-{Constrained} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.16264},
	doi = {10.48550/arXiv.2305.16264},
	abstract = {The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github.com/huggingface/datablations.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Muennighoff, Niklas and Rush, Alexander M. and Barak, Boaz and Scao, Teven Le and Piktus, Aleksandra and Tazi, Nouamane and Pyysalo, Sampo and Wolf, Thomas and Raffel, Colin},
	month = oct,
	year = {2023},
	note = {arXiv:2305.16264},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{jelassi_repeat_2024,
	title = {Repeat {After} {Me}: {Transformers} are {Better} than {State} {Space} {Models} at {Copying}},
	shorttitle = {Repeat {After} {Me}},
	url = {http://arxiv.org/abs/2402.01032},
	doi = {10.48550/arXiv.2402.01032},
	abstract = {Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as "generalized state space models" (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. Taken together, these results suggest a fundamental gap between transformers and GSSMs on tasks of practical interest.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Jelassi, Samy and Brandfonbrener, David and Kakade, Sham M. and Malach, Eran},
	month = jun,
	year = {2024},
	note = {arXiv:2402.01032},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{wang_mambabyte_2024,
	title = {{MambaByte}: {Token}-free {Selective} {State} {Space} {Model}},
	shorttitle = {{MambaByte}},
	url = {http://arxiv.org/abs/2401.13660},
	doi = {10.48550/arXiv.2401.13660},
	abstract = {Token-free language models learn directly from raw bytes and remove the inductive bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences. In this setting, standard autoregressive Transformers scale poorly as the effective memory required grows with sequence length. The recent development of the Mamba state space model (SSM) offers an appealing alternative approach with a fixed-sized memory state and efficient decoding. We propose MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences. In terms of modeling, we show MambaByte to be competitive with, and even to outperform, state-of-the-art subword Transformers on language modeling tasks while maintaining the benefits of token-free language models, such as robustness to noise. In terms of efficiency, we develop an adaptation of speculative decoding with tokenized drafting and byte-level verification. This results in a \$2.6{\textbackslash}times\$ inference speedup to the standard MambaByte implementation, showing similar decoding efficiency as the subword Mamba. These findings establish the viability of SSMs in enabling token-free language modeling.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Wang, Junxiong and Gangavarapu, Tushaar and Yan, Jing Nathan and Rush, Alexander M.},
	month = aug,
	year = {2024},
	note = {arXiv:2401.13660},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{gu_mamba_2024,
	title = {Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
	shorttitle = {Mamba},
	url = {http://arxiv.org/abs/2312.00752},
	doi = {10.48550/arXiv.2312.00752},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Gu, Albert and Dao, Tri},
	month = may,
	year = {2024},
	note = {arXiv:2312.00752},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{fu_flashfftconv_2023,
	title = {{FlashFFTConv}: {Efficient} {Convolutions} for {Long} {Sequences} with {Tensor} {Cores}},
	shorttitle = {{FlashFFTConv}},
	url = {http://arxiv.org/abs/2311.05908},
	doi = {10.48550/arXiv.2311.05908},
	abstract = {Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in \$O(N logN)\$ time in sequence length \$N\$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93\${\textbackslash}times\$ over PyTorch and achieves up to 4.4\${\textbackslash}times\$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1\% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50\%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Fu, Daniel Y. and Kumbong, Hermann and Nguyen, Eric and Ré, Christopher},
	month = nov,
	year = {2023},
	note = {arXiv:2311.05908},
	keywords = {Computer Science - Machine Learning},
}

@misc{leviathan_fast_2023,
	title = {Fast {Inference} from {Transformers} via {Speculative} {Decoding}},
	url = {http://arxiv.org/abs/2211.17192},
	doi = {10.48550/arXiv.2211.17192},
	abstract = {Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
	month = may,
	year = {2023},
	note = {arXiv:2211.17192},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{zhang_lolcats_2024,
	title = {{LoLCATs}: {On} {Low}-{Rank} {Linearizing} of {Large} {Language} {Models}},
	shorttitle = {{LoLCATs}},
	url = {http://arxiv.org/abs/2410.10254},
	doi = {10.48550/arXiv.2410.10254},
	abstract = {Recent works show we can linearize large language models (LLMs) -- swapping the quadratic attentions of popular Transformer-based LLMs with subquadratic analogs, such as linear attention -- avoiding the expensive pretraining costs. However, linearizing LLMs often significantly degrades model quality, still requires training over billions of tokens, and remains limited to smaller 1.3B to 7B LLMs. We thus propose Low-rank Linear Conversion via Attention Transfer (LoLCATs), a simple two-step method that improves LLM linearizing quality with orders of magnitudes less memory and compute. We base these steps on two findings. First, we can replace an LLM's softmax attentions with closely-approximating linear attentions, simply by training the linear attentions to match their softmax counterparts with an output MSE loss ("attention transfer"). Then, this enables adjusting for approximation errors and recovering LLM quality simply with low-rank adaptation (LoRA). LoLCATs significantly improves linearizing quality, training efficiency, and scalability. We significantly reduce the linearizing quality gap and produce state-of-the-art subquadratic LLMs from Llama 3 8B and Mistral 7B v0.1, leading to 20+ points of improvement on 5-shot MMLU. Furthermore, LoLCATs does so with only 0.2\% of past methods' model parameters and 0.4\% of their training tokens. Finally, we apply LoLCATs to create the first linearized 70B and 405B LLMs (50x larger than prior work). When compared with prior approaches under the same compute budgets, LoLCATs significantly improves linearizing quality, closing the gap between linearized and original Llama 3.1 70B and 405B LLMs by 77.8\% and 78.1\% on 5-shot MMLU.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Zhang, Michael and Arora, Simran and Chalamala, Rahul and Wu, Alan and Spector, Benjamin and Singhal, Aaryan and Ramesh, Krithik and Ré, Christopher},
	month = oct,
	year = {2024},
	note = {arXiv:2410.10254},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{zhang_hedgehog_2024,
	title = {The {Hedgehog} \& the {Porcupine}: {Expressive} {Linear} {Attentions} with {Softmax} {Mimicry}},
	shorttitle = {The {Hedgehog} \& the {Porcupine}},
	url = {http://arxiv.org/abs/2402.04347},
	doi = {10.48550/arXiv.2402.04347},
	abstract = {Linear attentions have shown potential for improving Transformer efficiency, reducing attention's quadratic complexity to linear in sequence length. This holds exciting promise for (1) training linear Transformers from scratch, (2) "finetuned-conversion" of task-specific Transformers into linear versions that recover task performance, and (3) "pretrained-conversion" of Transformers such as large language models into linear versions finetunable on downstream tasks. However, linear attentions often underperform standard softmax attention in quality. To close this performance gap, we find prior linear attentions lack key properties of softmax attention tied to good performance: low-entropy (or "spiky") weights and dot-product monotonicity. We further observe surprisingly simple feature maps that retain these properties and match softmax performance, but are inefficient to compute in linear attention. We thus propose Hedgehog, a learnable linear attention that retains the spiky and monotonic properties of softmax attention while maintaining linear complexity. Hedgehog uses simple trainable MLPs to produce attention weights mimicking softmax attention. Experiments show Hedgehog recovers over 99\% of standard Transformer quality in train-from-scratch and finetuned-conversion settings, outperforming prior linear attentions up to 6 perplexity points on WikiText-103 with causal GPTs, and up to 8.7 GLUE score points on finetuned bidirectional BERTs. Hedgehog also enables pretrained-conversion. Converting a pretrained GPT-2 into a linear attention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for 125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into a viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama2 7B achieves 28.1 higher ROUGE-1 points over the base standard attention model, where prior linear attentions lead to 16.5 point drops.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Zhang, Michael and Bhatia, Kush and Kumbong, Hermann and Ré, Christopher},
	month = feb,
	year = {2024},
	note = {arXiv:2402.04347},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{shah_flashattention-3_2024,
	title = {{FlashAttention}-3: {Fast} and {Accurate} {Attention} with {Asynchrony} and {Low}-precision},
	shorttitle = {{FlashAttention}-3},
	url = {http://arxiv.org/abs/2407.08608},
	doi = {10.48550/arXiv.2407.08608},
	abstract = {Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. FlashAttention elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in recent hardware, with FlashAttention-2 achieving only 35\% utilization on the H100 GPU. We develop three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. We demonstrate that our method, FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0\${\textbackslash}times\$ with FP16 reaching up to 740 TFLOPs/s (75\% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate that FP8 FlashAttention-3 achieves 2.6\${\textbackslash}times\$ lower numerical error than a baseline FP8 attention.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Shah, Jay and Bikshandi, Ganesh and Zhang, Ying and Thakkar, Vijay and Ramani, Pradeep and Dao, Tri},
	month = jul,
	year = {2024},
	note = {arXiv:2407.08608},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{bick_transformers_2024,
	title = {Transformers to {SSMs}: {Distilling} {Quadratic} {Knowledge} to {Subquadratic} {Models}},
	shorttitle = {Transformers to {SSMs}},
	url = {http://arxiv.org/abs/2408.10189},
	abstract = {Transformer architectures have become a dominant paradigm for domains like language modeling but suffer in many inference settings due to their quadratic-time self-attention. Recently proposed subquadratic architectures, such as Mamba, have shown promise, but have been pretrained with substantially less computational resources than the strongest Transformer models. In this work, we present a method that is able to distill a pretrained Transformer architecture into alternative architectures such as state space models (SSMs). The key idea to our approach is that we can view both Transformers and SSMs as applying different forms of mixing matrices over the token sequences. We can thus progressively distill the Transformer architecture by matching different degrees of granularity in the SSM: first matching the mixing matrices themselves, then the hidden units at each block, and finally the end-to-end predictions. Our method, called MOHAWK, is able to distill a Mamba-2 variant based on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens and a hybrid version (Hybrid Phi-Mamba) using 5B tokens. Despite using less than 1\% of the training data typically used to train models from scratch, Phi-Mamba boasts substantially stronger performance compared to all past open-source non-Transformer models. MOHAWK allows models like SSMs to leverage computational resources invested in training Transformer-based architectures, highlighting a new avenue for building such models.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Bick, Aviv and Li, Kevin Y. and Xing, Eric P. and Kolter, J. Zico and Gu, Albert},
	month = aug,
	year = {2024},
	note = {arXiv:2408.10189},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}
